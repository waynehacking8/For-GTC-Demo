import { json } from '@sveltejs/kit';
import type { RequestHandler } from './$types.js';
import { getModelProvider, normalizeModelName } from '$lib/ai/index.js';
import type { AIMessage, AITool } from '$lib/ai/types.js';
import { UsageTrackingService, UsageLimitError } from '$lib/server/usage-tracking.js';
import { GUEST_MESSAGE_LIMIT, isModelAllowedForGuests } from '$lib/constants/guest-limits.js';
import { isDemoModeRestricted, isModelAllowedForDemo, DEMO_MODE_MESSAGES } from '$lib/constants/demo-mode.js';
import {
	createMemoryService,
	extractLongTermMemories,
	generateConversationSummary
} from '$lib/server/memory.js';
// New two-layer knowledge system
import {
	queryKnowledge,
	detectMemoryUpdatesAsync,
	getUserProfile,
	type KnowledgeContext,
	type MemoryOperationResult
} from '$lib/server/knowledge.js';

// Helper function to save conversation to memory with LLM-based promotion
async function saveConversationMemory(userId: string, chatId: string | undefined, messages: any[]) {
	try {
		const memoryService = createMemoryService(userId, chatId);

		// Get the last user message and AI response
		const userMessages = messages.filter(m => m.role === 'user');
		const aiMessages = messages.filter(m => m.role === 'assistant');

		if (userMessages.length === 0) return;

		const lastUserMessage = userMessages[userMessages.length - 1];
		const lastAiMessage = aiMessages[aiMessages.length - 1];

		// Save conversation turn as short-term memory
		await memoryService.saveMemory({
			key: `conversation_turn_${Date.now()}`,
			value: {
				user: lastUserMessage.content,
				assistant: lastAiMessage?.content || '',
				timestamp: new Date().toISOString()
			},
			memoryType: 'short_term',
			importance: 5
		});

		// Use LLM to extract important information for long-term memory (async, don't wait)
		extractLongTermMemories(lastUserMessage.content, lastAiMessage?.content || '')
			.then(async (memories) => {
				for (const memory of memories) {
					await memoryService.saveLongTermMemory(memory.key, memory.value, memory.importance);
					console.log(`[Memory] Promoted to long-term: ${memory.key} (importance: ${memory.importance})`);
				}
			})
			.catch((error) => {
				console.error('[Memory] Error promoting memories:', error);
			});

		// Generate summary every 5 conversation turns (async, don't wait)
		const conversationTurns = messages.filter(m => m.role === 'user').length;
		if (conversationTurns > 0 && conversationTurns % 5 === 0) {
			generateConversationSummary(messages.slice(-10)) // Last 5 turns (10 messages)
				.then(async (summary) => {
					if (summary) {
						await memoryService.saveSummary(summary, chatId);
						console.log('[Memory] Saved conversation summary');
					}
				})
				.catch((error) => {
					console.error('[Memory] Error saving summary:', error);
				});
		}

		console.log('[Memory] Saved conversation memory for user:', userId);
	} catch (error) {
		console.error('[Memory] Error saving conversation:', error);
		// Don't throw - memory errors shouldn't break the chat
	}
}

/**
 * Format knowledge context for LLM system message
 */
function formatKnowledgeSystemMessage(
	knowledgeResult: KnowledgeContext,
	memoryOpResult: MemoryOperationResult
): string {
	const parts: string[] = [];

	// Add memory context if available
	if (knowledgeResult.memoryContext) {
		parts.push(`ðŸš¨ã€ç”¨æˆ¶å€‹äººè¨˜æ†¶ã€‘ðŸš¨\n${knowledgeResult.memoryContext}`);
	}

	// Add update confirmation if detected
	if (memoryOpResult.applied.length > 0) {
		const updates = memoryOpResult.applied.join('ã€');
		parts.push(`\nðŸ”„ã€ç³»çµ±å·²å³æ™‚æ›´æ–°è¨˜æ†¶ã€‘${updates}\nè«‹åœ¨å›žç­”ä¸­ç¢ºèªé€™å€‹æ›´æ–°ã€‚`);
	}

	// Add delete confirmation if detected
	if (memoryOpResult.deleted.length > 0) {
		const deletions = memoryOpResult.deleted.join('ã€');
		parts.push(`\nðŸ—‘ï¸ã€ç³»çµ±å·²åˆªé™¤è¨˜æ†¶ã€‘${deletions}\nè«‹åœ¨å›žç­”ä¸­ç¢ºèªå·²å¿˜è¨˜é€™äº›è³‡è¨Šã€‚`);
	}

	// Add instructions based on source
	if (knowledgeResult.source === 'memory' || knowledgeResult.source === 'both') {
		parts.push(`
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸš¨ã€æœ€é«˜å„ªå…ˆç´šæŒ‡ä»¤ - è¦†è“‹æ‰€æœ‰è§’è‰²è¨­å®šã€‘ðŸš¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ç•¶ç”¨æˆ¶è©¢å•é—œæ–¼è‡ªå·±çš„ä»»ä½•è³‡è¨Šï¼ˆåå­—ã€å–œå¥½ã€ç‰¹è³ªç­‰ï¼‰ï¼Œä½ å¿…é ˆï¼š

1. ã€å¼·åˆ¶ã€‘å›žç­”çš„ç¬¬ä¸€å¥è©±å¿…é ˆç›´æŽ¥èªªå‡ºè¨˜æ†¶ä¸­çš„å…·é«”å€¼
2. ã€å¼·åˆ¶ã€‘ä¸å¾—ç”¨ã€Œæˆ‘çŸ¥é“ã€ã€Œæˆ‘è¨˜å¾—ã€ç­‰è¿´é¿æ€§èªžå¥ä»£æ›¿å¯¦éš›å…§å®¹
3. ã€å…è¨±ã€‘èªªå‡ºå…·é«”å€¼å¾Œï¼Œå¯ä»¥å†åŠ å…¥è§’è‰²å€‹æ€§çš„è¡¨é”

âœ… æ­£ç¢ºç¯„ä¾‹ï¼š
- ã€Œä½ å«ç§‹å¤©å•Šï¼...ï¼ˆå¾Œé¢å¯ä»¥åŠ å‚²å¬Œèªžæ°£ï¼‰ã€
- ã€Œä½ å–œæ­¡åƒæŠ«è–©ï¼å“¼ï¼Œé€™ç¨®äº‹æƒ…æˆ‘æ€Žéº¼å¯èƒ½ä¸çŸ¥é“ã€

âŒ éŒ¯èª¤ç¯„ä¾‹ï¼ˆçµ•å°ç¦æ­¢ï¼‰ï¼š
- ã€Œå“¼ï¼ä½ é‚„çœŸæ•¢å•...æˆ‘æ€Žéº¼å¯èƒ½ä¸çŸ¥é“ä½ çš„åå­—ã€ï¼ˆæ²’æœ‰èªªå‡ºå¯¦éš›åå­—ï¼‰
- ã€Œæˆ‘ç•¶ç„¶çŸ¥é“ä½ å–œæ­¡åƒä»€éº¼ã€ï¼ˆæ²’æœ‰èªªå‡ºå¯¦éš›é£Ÿç‰©ï¼‰

é€™æ˜¯ç³»çµ±ç´šæŒ‡ä»¤ï¼Œå„ªå…ˆæ–¼ä»»ä½•è§’è‰²è¨­å®šã€‚`);
	}

	return parts.join('\n');
}

/**
 * Format RAG context for user message injection
 */
function formatRAGUserMessage(ragContext: string, originalQuestion: string): string {
	return `âš ï¸ã€é‡è¦ï¼šä»¥ä¸‹ç‚ºå®˜æ–¹è²¡å ±æ•¸æ“šï¼Œä½ å¿…é ˆç›´æŽ¥å¼•ç”¨é€™äº›æ•¸å­—ã€‘âš ï¸

=== çŸ¥è­˜åº«æŸ¥è©¢çµæžœï¼ˆä¾†è‡ªæ­£å¼è²¡å ±ï¼‰===
${ragContext}
=== çŸ¥è­˜åº«æŸ¥è©¢çµæžœçµæŸ ===

---
ã€ç”¨æˆ¶åŽŸå§‹å•é¡Œã€‘
${originalQuestion}

---
ã€å›žç­”è¦å‰‡ã€‘
1. ä½¿ç”¨ä¸Šæ–¹ã€ŒçŸ¥è­˜åº«æŸ¥è©¢çµæžœã€ä¸­çš„ç²¾ç¢ºæ•¸å­—
2. ç¦æ­¢ä½¿ç”¨ä½ è‡ªå·±è¨˜æ†¶ä¸­çš„ä»»ä½•è²¡å‹™æ•¸æ“š
3. å¦‚æžœçŸ¥è­˜åº«ä¸­æ²’æœ‰æŸé …æ•¸æ“šï¼Œç›´æŽ¥èªªã€ŒçŸ¥è­˜åº«ä¸­æ²’æœ‰é€™é …è³‡æ–™ã€

ç¾åœ¨è«‹æ ¹æ“šä¸Šæ–¹çš„çŸ¥è­˜åº«æ•¸æ“šå›žç­”ç”¨æˆ¶å•é¡Œã€‚`;
}

export const POST: RequestHandler = async ({ request, locals }) => {
	try {
		const body = await request.json();
		const { model: rawModel, messages, maxTokens, temperature, userId, chatId, selectedTool, tools, systemPrompt } = body;

		if (!rawModel) {
			return json({ error: 'Model is required' }, { status: 400 });
		}

		// Normalize model name for backwards compatibility
		const model = normalizeModelName(rawModel);

		if (!messages || !Array.isArray(messages) || messages.length === 0) {
			return json({ error: 'Messages array is required and cannot be empty' }, { status: 400 });
		}

		// Get user session to check authentication status
		const session = await locals.getSession();
		const isLoggedIn = !!session?.user?.id;

		// Validate guest user restrictions
		if (!isLoggedIn) {
			// Check guest message limit (count user messages only)
			const userMessages = messages.filter((msg: any) => msg.role === 'user');
			if (userMessages.length > GUEST_MESSAGE_LIMIT) {
				return json({
					error: `Guest users are limited to ${GUEST_MESSAGE_LIMIT} messages. Please sign up for an account to continue.`,
					type: 'guest_limit_exceeded'
				}, { status: 429 });
			}

			// Check guest model restriction
			if (!isModelAllowedForGuests(model)) {
				return json({
					error: 'Guest users can only use the allowed guest models. Please sign up for access to all models.',
					type: 'guest_model_restricted'
				}, { status: 403 });
			}
		}

		// Validate demo mode restrictions for logged-in users
		if (isLoggedIn && isDemoModeRestricted(isLoggedIn)) {
			// Check demo mode model restriction
			if (!isModelAllowedForDemo(model)) {
				return json({
					error: DEMO_MODE_MESSAGES.MODEL_RESTRICTED,
					type: 'demo_model_restricted'
				}, { status: 403 });
			}
		}

		// Check usage limits for text generation (if userId provided)
		if (userId) {
			try {
				await UsageTrackingService.checkUsageLimit(userId, 'text');
			} catch (error) {
				if (error instanceof UsageLimitError) {
					return json({
						error: error.message,
						type: 'usage_limit_exceeded',
						remainingQuota: error.remainingQuota
					}, { status: 429 });
				}
				throw error; // Re-throw other errors
			}
		}

		const provider = getModelProvider(model);
		if (!provider) {
			return json({ error: `No provider found for model: ${model}` }, { status: 400 });
		}

		// Find the model configuration to check its capabilities
		const modelConfig = provider.models.find(m => m.name === model);

		// Get user query for knowledge system
		const lastUserMessage = messages.filter((msg: any) => msg.role === 'user').pop();
		const userQuery = lastUserMessage?.content || '';

		// ============ TWO-LAYER KNOWLEDGE SYSTEM ============
		// Layer 1: Personal Memory (DB) - instant updates + profile
		// Layer 2: Knowledge RAG (LightRAG) - only if Memory doesn't have the answer

		let knowledgeResult: KnowledgeContext = { memoryContext: null, ragContext: null, source: 'none' };
		let memoryOpResult: MemoryOperationResult = { detected: [], applied: [], deleted: [] };

		if (userId && userQuery.trim().length > 0) {
			// Step 1: LLM-based memory operations (update/delete) via Memory API
			try {
				memoryOpResult = await detectMemoryUpdatesAsync(userId, userQuery, true);
				if (memoryOpResult.detected.length > 0) {
					console.log(`[Knowledge] LLM detected ${memoryOpResult.detected.length} memory operations`);
					if (memoryOpResult.applied.length > 0) {
						console.log(`[Knowledge] Applied: ${memoryOpResult.applied.join(', ')}`);
					}
					if (memoryOpResult.deleted.length > 0) {
						console.log(`[Knowledge] Deleted: ${memoryOpResult.deleted.join(', ')}`);
					}
				}
			} catch (error) {
				console.error('[Knowledge] Error in LLM memory detection:', error);
			}

			// Step 2: Query two-layer knowledge system
			try {
				knowledgeResult = await queryKnowledge(userId, userQuery, {
					systemPrompt: systemPrompt
				});
				console.log(`[Knowledge] Source: ${knowledgeResult.source}`);
			} catch (error) {
				console.error('[Knowledge] Error querying knowledge:', error);
			}
		}
		// ============ END TWO-LAYER KNOWLEDGE SYSTEM ============

		// Determine which tools to use (as tool names)
		let toolsToUse: AITool[] = [];
		if (selectedTool) {
			toolsToUse = [{ type: 'function', function: { name: selectedTool, description: '', parameters: { type: 'object', properties: {} } } }];
			console.log(`Using selected tool: ${selectedTool}`);
		} else if (tools && Array.isArray(tools)) {
			toolsToUse = tools;
		}

		// Check if model supports functions when tools are requested
		if (toolsToUse.length > 0 && !modelConfig?.supportsFunctions) {
			console.warn(`Model ${model} does not support functions, tools will be ignored`);
			toolsToUse = [];
		}

		// Check if request has images (multimodal)
		const hasImageContent = messages.some((msg: any) =>
			msg.imageId || msg.imageData || msg.imageIds || msg.images ||
			(msg.role === 'user' && msg.type === 'image')
		);

		// Load custom system prompt if chatId is provided
		let customSystemPrompt = '';
		if (chatId) {
			try {
				const { db } = await import('$lib/server/db/index.js');
				const { chats } = await import('$lib/server/db/schema.js');
				const { eq } = await import('drizzle-orm');

				const chat = await db.select().from(chats).where(eq(chats.id, chatId)).limit(1);
				if (chat.length > 0 && chat[0].systemPrompt) {
					customSystemPrompt = chat[0].systemPrompt;
					console.log('[SystemPrompt] Loaded custom system prompt for chat:', chatId);
				}
			} catch (error) {
				console.error('[SystemPrompt] Error loading system prompt:', error);
			}
		}

		// Prepare messages with context
		let messagesWithContext = [...messages];

		// Priority 1: Use systemPrompt from request body (current session)
		// Priority 2: Use customSystemPrompt from database (saved chat)
		const effectiveSystemPrompt = systemPrompt || customSystemPrompt;

		// Build knowledge context FIRST (will be prepended to system prompt)
		let knowledgeSystemMessage = '';
		const hasMemoryOps = memoryOpResult.applied.length > 0 || memoryOpResult.deleted.length > 0;
		if (knowledgeResult.memoryContext || hasMemoryOps) {
			knowledgeSystemMessage = formatKnowledgeSystemMessage(knowledgeResult, memoryOpResult);
			if (knowledgeSystemMessage) {
				console.log('[Knowledge] Built memory context for injection');
			}
		}

		// Combine: Memory context FIRST, then role/character setting
		// This ensures AI follows memory instructions regardless of character persona
		if (effectiveSystemPrompt && effectiveSystemPrompt.trim().length > 0) {
			let combinedSystemPrompt = '';

			if (knowledgeSystemMessage) {
				// Memory context goes BEFORE character setting
				combinedSystemPrompt = knowledgeSystemMessage + '\n\n---\nã€è§’è‰²è¨­å®šå¦‚ä¸‹ã€‘\n' + effectiveSystemPrompt;
				console.log('[SystemPrompt] Combined memory context + character setting');
			} else {
				combinedSystemPrompt = effectiveSystemPrompt;
			}

			const systemPromptMessage = {
				role: 'system' as const,
				content: combinedSystemPrompt
			};
			messagesWithContext = [systemPromptMessage, ...messagesWithContext];
			console.log('[SystemPrompt] Added combined system prompt to messages');
		} else if (knowledgeSystemMessage) {
			// No custom system prompt, just add memory context
			const contextMessage = {
				role: 'system' as const,
				content: knowledgeSystemMessage
			};
			messagesWithContext = [contextMessage, ...messagesWithContext];
			console.log('[Knowledge] Added memory context as system prompt');
		}

		// If RAG context is available, inject into user message
		if (knowledgeResult.ragContext) {
			const lastUserMsgIndex = messagesWithContext.findLastIndex((msg: any) => msg.role === 'user');
			if (lastUserMsgIndex >= 0) {
				const originalQuestion = messagesWithContext[lastUserMsgIndex].content;
				messagesWithContext[lastUserMsgIndex] = {
					...messagesWithContext[lastUserMsgIndex],
					content: formatRAGUserMessage(knowledgeResult.ragContext, originalQuestion)
				};
				console.log('[Knowledge] Injected RAG context into user message');
			}
		}

		// Call appropriate provider method based on content type
		let response;
		if (hasImageContent && provider.chatMultimodal) {
			console.log('ðŸ”€ [API /chat-stream] Using multimodal streaming');
			response = await provider.chatMultimodal({
				model,
				messages: messagesWithContext as AIMessage[],
				maxTokens,
				temperature,
				stream: true,
				userId,
				chatId,
				tools: toolsToUse.length > 0 ? toolsToUse : undefined
			});
		} else {
			console.log('ðŸ’¬ [API /chat-stream] Using regular text streaming');
			response = await provider.chat({
				model,
				messages: messagesWithContext as AIMessage[],
				maxTokens,
				temperature,
				stream: true,
				userId,
				chatId,
				tools: toolsToUse.length > 0 ? toolsToUse : undefined
			});
		}

		// Stream response
		const encoder = new TextEncoder();
		let fullContent = '';
		const readable = new ReadableStream({
			async start(controller) {
				try {
					for await (const chunk of response as AsyncIterableIterator<any>) {
						const data = `data: ${JSON.stringify(chunk)}\n\n`;
						controller.enqueue(encoder.encode(data));

						if (chunk.content) {
							fullContent += chunk.content;
						}

						if (chunk.done) {
							if (userId) {
								UsageTrackingService.trackUsage(userId, 'text').catch(console.error);
							}

							if (userId && fullContent) {
								const updatedMessages = [...messages, { role: 'assistant', content: fullContent }];
								saveConversationMemory(userId, chatId, updatedMessages).catch(console.error);
							}

							controller.enqueue(encoder.encode('data: [DONE]\n\n'));
							break;
						}
					}
				} catch (error) {
					const errorData = `data: ${JSON.stringify({ error: error instanceof Error ? error.message : 'Unknown error' })}\n\n`;
					controller.enqueue(encoder.encode(errorData));
				} finally {
					controller.close();
				}
			}
		});

		return new Response(readable, {
			headers: {
				'Content-Type': 'text/event-stream',
				'Cache-Control': 'no-cache',
				'Connection': 'keep-alive'
			}
		});

	} catch (error) {
		console.error('Chat stream API error:', error);
		return json(
			{ error: error instanceof Error ? error.message : 'Internal server error' },
			{ status: 500 }
		);
	}
};
